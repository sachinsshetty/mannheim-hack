services:
  ollama:
    volumes:
      - ~/ollama/ollama:/root/.ollama
    container_name: ollama
    #pull_policy: always
    tty: true
    #restart: unless-stopped
    image: ollama/ollama:latest
    ports:
      - 11434:11434
    networks:
      - app-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
  backend:
    build: ./backend  # Assuming your backend code is in a directory named 'backend'
    container_name: backend
    ports:
      - 5000:5000  # Map the container port to the host
    depends_on:
      - ollama  # Ensure that the ollama service is started before the backend service
    networks:
      - app-network
#  frontend:
#    image: slabstech/mannheim-ui
#    container_name: frontend-gpu
#    ports:
#      - 8000:80  # Map the container port to the host
#    depends_on:
#      - ollama  # Ensure that the ollama service is started before the frontend service
#    extra_hosts:
#      - host.docker.internal:host-gateway
#    environment:
#      - 'VITE_OLLAMA_BASE_URL=http://localhost:11435/api'
#    networks:
#      - app-network
networks:
  app-network:
    driver: bridge